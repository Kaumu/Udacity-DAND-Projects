{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRatedogs Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 steps reqired in the wrangling process of the data\n",
    "\n",
    "    I.   Gathering\n",
    "    II.  Assessing\n",
    "    III. Cleaning\n",
    "    IV.  Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different pieces of data. The gathering of data individually was performed differently. \n",
    "\n",
    "- The WeRateDogs Twitter archive was provided by the account holder, only uploading and reading of the data (twitter-archive-enhanced.csv) was done using Pandas. df_twitter data frame was created to store the data.\n",
    "\n",
    "\n",
    "- Second data was present in an HTML structure so the requests library was used to get the data and save it in a flat file structure (image-predictions.tsv). df_image data frame was created to store the image predictions data.\n",
    "\n",
    "\n",
    "- The final data was gathered using the Twitter API, Tweepy. The data was pulled based on the tweet_id and then written on a text file (tweet_json.txt). The data was stored in the df_tweetapi data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  **Assesing the df_twitter data:**\n",
    " \n",
    "       The Twitter data consists of about 2356 rows and 17 columns. This table consists of columns likes\n",
    "             \n",
    "      **tweet_id** - id of the tweet\n",
    "      \n",
    "      **in_reply_to_status_id** -   status id of the reply given to the tweet id  \n",
    "      \n",
    "      **in_reply_to_user_id** - user id of the reply given to the tweet id\n",
    "      \n",
    "      **timestamp** - The time at which the tweet was posted\n",
    "      \n",
    "      **source** - The web link for the source\n",
    "      \n",
    "      **text** - The text of the tweet\n",
    "      \n",
    "      **retweeted_status_id** - The status id of the reply to the tweet\n",
    "      \n",
    "      **retweeted_status_user_id** - The user id of the reply given to the tweet id\n",
    "      \n",
    "      **retweeted_status_timestamp** - The time at which the retweet was posted\n",
    "      \n",
    "      **expanded_urls** - The expanded URL of the URL in the tweet\n",
    "      \n",
    "      **rating_numerator** - The numerator values of the rating\n",
    "      \n",
    "      **rating_denominator** - The denominator value of the rating\n",
    "      \n",
    "      **name** - The name of the dog\n",
    "      \n",
    "      **doggo** - The dog stage of the dog \n",
    "      \n",
    "      **floofer** - The dog stage of the dog\n",
    "      \n",
    "      **pupper** - The dog stage of the dog\n",
    "      \n",
    "      **puppo** - The dog stage of the dog\n",
    "\n",
    "       Data is visually and programmatically assessed for any data discrepancies. We can check if the data has any null values or duplicated values for certain columns. Also, all the data that is not required for our analysis can be dropped off for easy storage and also access. The names of the dogs are checked to view if they have anything other than the proper names. Dog stages are checked if each is allotted only one stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  **Assesing the df_image data:**\n",
    " \n",
    "       The Twitter data consists of about 2075 rows and 12 columns. This table consists of columns likes\n",
    "             \n",
    "      **tweet_id** - id of the tweet\n",
    "      \n",
    "      **jpg_url** -   the URL of the image \n",
    "      \n",
    "      **img_num** - number of images\n",
    "      \n",
    "      **p1** - algorithm's number 1 prediction\n",
    "      \n",
    "      **p1_conf** - algorithm's number 1 prediction confidence\n",
    "      \n",
    "      **p1_dog** - Is prediction 1 true or false\n",
    "      \n",
    "      **p2** - algorithm's number 2 prediction\n",
    "      \n",
    "      **p2_conf** - algorithm's number 2 prediction confidence\n",
    "      \n",
    "      **p2_dog** - Is prediction 2 true or false\n",
    "      \n",
    "      **p3** - algorithm's number 3 prediction\n",
    "      \n",
    "      **p3_conf** - algorithm's number 3 prediction confidence\n",
    "      \n",
    "      **p3_dog** - Is prediction 2 true or false\n",
    "      \n",
    "    The data is assessed to see if the data has any null values or duplicate values. The data type of the columns is checked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Assesing the df_tweetapi data:**\n",
    " \n",
    "     The Twitter data consists of about 1403 rows and 3 columns. This table consists of columns likes\n",
    "             \n",
    "     **tweet_id** - id of the tweet\n",
    "      \n",
    "     **retweet_count** -   count of the retweets for the tweet_id\n",
    "      \n",
    "     **favorite_count** - count of the favorites for the tweet_id\n",
    "      \n",
    "     The data is analyzed for the datatype and any null values or duplicated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Cleaning of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Quality and Tidiness issues in the three data frames.\n",
    "\n",
    "#### Quality Issues\n",
    "\n",
    "For `df_twitter` data - \n",
    "\n",
    "1. The datatype should be changed for the tweet_id from into string. Along with the timestamp date should be changed from string to datetime.\n",
    "\n",
    "2. Few of the dog names have no proper names and have values like a, my, old, by, and so on. Also, the null values are named None instead of NaN.\n",
    "\n",
    "3. Retweet data is not required so we could just drop the columns which has retweet information.\n",
    "\n",
    "4. There is one denominator that has a 0 value. We need to change this to 1\n",
    "\n",
    "5. A new column with a rating can be created to have a final value based on the numerator rating and denominator rating.\n",
    "\n",
    "For `df_image` data - \n",
    "\n",
    "6. The datatype of tweet_id should be changed from int to string. Also, the img_num should be a category as there are at max 4 images.\n",
    "\n",
    "7. There are duplicated records in the jpr_url column, so we need to drop the duplicates\n",
    "\n",
    "8. p1, p2, and p3 columns have underscores in between dog breed names. So we could just replace them with space. \n",
    "\n",
    "For `df_twitterapi` data - \n",
    "\n",
    "9. The datatype of tweet_id should be changed from int to string.\n",
    "\n",
    "\n",
    "#### Tidiness Issues\n",
    "\n",
    "1. The dog stage has 4 different columns in the `df_twitter` data frame. We can just reduce them to one column with the type of breed and make it categorical data. \n",
    "\n",
    "2. We can merge all three tables to have better access to the different data in one place and also drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data after all the cleaning and merging is stored in a CSV format. (twitter_archive_master.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
